Imagine you're a painter, with a canvas and a set of paints. You're told to paint a scene, but you're not allowed to look at the canvas while you paint; you instead have to look at the scene *for just one second*, turn around, then recreate the scene on your canvas from memory. 

As a result, you start with broad strokes capturing the important details, and as you turn around and look at the canvas more, you get a better sense of the scene and can adjust the painting accordingly.

This process of distilling a grand scene into its core essence is quite beautiful. In Machine Learning, this study is formalized as *dimensionality reduction*, i.e. the process of reducing the noise in complex data. An *autoencoder*, visualized here, is one such form of dimensionality reduction. This specific one is heavily inspired by [ConvNetJS](https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html), a project by Andrej Karpathy.

It’s captivating to visualize the process of perception, but especially as it relates to humans vs AI. 

With humans, as time goes on, the sharp details of the memory get eroded by its associated sentiments. The memory persists more as a collection of moments and feelings than a detailed and exact replay.

When I see an image from the past, both this autoencoder and my brain start a reconstruction process. A picture is repainted in my head and with the autoencoder, blurry in some details, yet brilliantly vivid in others.

The part that makes us human though, is that unlike the AI, my head isn’t computing an exact reconstruction of the image. My head remembers the way girlfriend laughs at my dumb jokes and curses me for rotting her brain. It remembers the night my friends and I were drunk and out of toilet paper at 2am and stealthily stole heaps of TP from a nearby McDonald’s to bring back to the group. 

Some memories are just better that way — inexact, imperfect, but vivid in the ways you reconstruct them.